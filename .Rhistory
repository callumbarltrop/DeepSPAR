packages = c("keras","tensorflow","reticulate")
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
py_version <- "3.10.17"
path_to_python <- reticulate::install_python(version=py_version)
py_version <- "3.10.14"
path_to_python <- reticulate::install_python(version=py_version)
reticulate::py_available()
reticulate::py_config()
reticulate::install_python("3.10.17")
reticulate::install_python("3.10")
reticulate::virtualenv_create(envname = 'DeepSPAR_env',
python=path_to_python,
version=py_version)
path_to_python <- reticulate::install_python("3.10")
reticulate::virtualenv_create(envname = 'DeepSPAR_env',
python=path_to_python,
version=py_version)
reticulate::use_virtualenv("DeepSPAR_env", required = T)
tf_version="2.11.0"
reticulate::use_virtualenv("DeepSPAR_env", required = T)
tensorflow::install_tensorflow(method="virtualenv", envname="DeepSPAR_env",
version=tf_version)
reticulate::use_virtualenv("DeepSPAR_env", required = T)
keras::install_keras(method = c("virtualenv"), envname = "DeepSPAR_env",version=tf_version) #Install keras
reticulate::use_virtualenv("DeepSPAR_env", required = T)
keras::is_keras_available() #Check if keras is available
quant.nunits = c(16,16,16)
quant.level = 0.9
gpd.nunits = c(16,16,16)
# Load some useful functions and packages
source("preamble.R")
setwd("D:/Github/DeepSPAR")
# Load some useful functions and packages
source("preamble.R")
# Load some useful functions and packages
source("preamble.R")
# Load tensorflow/Keras and Virtual Environment
reticulate::use_virtualenv("DeepSPAR_env", required = T)
packages = c("keras", "tensorflow","reticulate")
package.check <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
library(tensorflow)
# Set tensorflow seed
set_random_seed(1)
quant.nunits = c(16,16,16)
quant.level = 0.9
gpd.nunits = c(16,16,16)
l.r.vec = c(0.001, 5e-04,  1e-04)
df <- read.csv("Data/wind_wave_data.csv")
X <- as.matrix(df[, c(5:6, 7:8, 9)])  # Period must always be last dimension!!
X[, 5] <- log(X[, 5])
means = apply(X, 2, mean)
means[1:4]=0
sds = apply(X, 2, sd)
X.norm = X
for (i in 1:ncol(X)) X.norm[, i] = (X[, i] - means[i])/sds[i]
dev.new()
dev.new()
plot(X.norm[1:1000,])
pairs(X.norm[1:1000,])
X.norm = X
for (i in 1:ncol(X)) X.norm[, i] = (X[, i] - means[i])/sds[i]
pairs(X.norm[1:1000,])
pairs(X.norm[1:3000,])
quant.nunits = c(16,16,16)
quant.level = 0.9
gpd.nunits = c(16,16,16)
l.r.vec = c(0.001, 5e-04,  1e-04)
df <- read.csv("Data/wind_wave_data.csv")
X <- as.matrix(df[, c(5:6, 7:8, 9)])
X[, 5] <- log(X[, 5])
means = apply(X, 2, mean)
means[1:4]=0
sds = apply(X, 2, sd)
X.norm = X
for (i in 1:ncol(X)) X.norm[, i] = (X[, i] - means[i])/sds[i]
polar = rect2polar(t(X.norm))  #Get polar coordinates
n = dim(X.norm)[1]
d = dim(X.norm)[2]
# Create AR decomp
R = polar$r
W = X.norm/R
W = as.matrix(W)
# Make 20% validation data
set.seed(1)
valid.inds = sample(1:n, round(n/10))
test.inds = sample((1:n)[-valid.inds], round(n/10))
activation.func = "relu"
R.train <- R[-valid.inds]
W.train <- W[-valid.inds,]
R.valid <- R[valid.inds]
W.valid <- W[valid.inds,]
R.test <- R[test.inds]
W.test <- W[test.inds,]
input_pseudo_angles <- layer_input(shape = d, name = "input_pseudo_angles")
qBranch <- input_pseudo_angles %>%
layer_dense(units = quant.nunits[1], activation = activation.func, name = "q_dense1", kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))
for (i in 2:length(quant.nunits)) {
qBranch <- qBranch %>%
layer_dense(units = quant.nunits[i], activation = activation.func, name = paste0("q_dense", i), kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))
}
qBranch <- qBranch %>%
layer_dense(units = 1, activation = "exponential", name = "q_final", kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))
# Construct Keras model
quant.model <- keras_model(inputs = c(input_pseudo_angles), outputs = c(qBranch))
# Compile the model with the tilted loss and the adam optimiser
quant.model %>%
compile(optimizer = "adam", loss = tilted_loss, run_eagerly = T)
checkpoint <- callback_model_checkpoint(filepath = paste0("runs/QR_est/qr_fit_arch_opt_2d_",array_id), monitor = "val_loss", verbose = 0,
save_best_only = TRUE, save_weights_only = TRUE, mode = "min", save_freq = "epoch")
checkpoint <- callback_model_checkpoint(filepath = paste0("runs/QR_est/qr_fit_q_",quant.level), monitor = "val_loss", verbose = 0,
save_best_only = TRUE, save_weights_only = TRUE, mode = "min", save_freq = "epoch")
n.epochs <- 512
batch.size <- 1024
dim(R)=c(length(R),1); dim(R.train)=c(length(R.train),1); dim(R.valid)=c(length(R.valid),1);dim(R.test)=c(length(R.test),1)
history <- quant.model %>%
fit(list(W.train), R.train, epochs = n.epochs, batch_size = batch.size, callback = list(checkpoint,
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 5)), validation_data = list(list(input_pseudo_angles = W.valid),R.valid))
# Load some useful functions and packages
source("preamble.R")
# Load tensorflow/Keras and Virtual Environment
reticulate::use_virtualenv("DeepSPAR_env", required = T)
packages = c("keras", "tensorflow","reticulate")
package.check <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
library(tensorflow)
# Set tensorflow seed
set_random_seed(1)
quant.nunits = c(16,16,16)
quant.level = 0.9
gpd.nunits = c(16,16,16)
l.r.vec = c(0.001, 5e-04,  1e-04)
df <- read.csv("Data/wind_wave_data.csv")
X <- as.matrix(df[, c(5:6, 7:8, 9)])
X[, 5] <- log(X[, 5])
means = apply(X, 2, mean)
means[1:4]=0
sds = apply(X, 2, sd)
X.norm = X
for (i in 1:ncol(X)) X.norm[, i] = (X[, i] - means[i])/sds[i]
polar = rect2polar(t(X.norm))  #Get polar coordinates
n = dim(X.norm)[1]
d = dim(X.norm)[2]
# Create AR decomp
R = polar$r
W = X.norm/R
W = as.matrix(W)
# Make 20% validation data
set.seed(1)
valid.inds = sample(1:n, round(n/10))
test.inds = sample((1:n)[-valid.inds], round(n/10))
activation.func = "relu"
R.train <- R[-valid.inds]
W.train <- W[-valid.inds,]
R.valid <- R[valid.inds]
W.valid <- W[valid.inds,]
R.test <- R[test.inds]
W.test <- W[test.inds,]
input_pseudo_angles <- layer_input(shape = d, name = "input_pseudo_angles")
qBranch <- input_pseudo_angles %>%
layer_dense(units = quant.nunits[1], activation = activation.func, name = "q_dense1", kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))
for (i in 2:length(quant.nunits)) {
qBranch <- qBranch %>%
layer_dense(units = quant.nunits[i], activation = activation.func, name = paste0("q_dense", i), kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))
}
qBranch <- qBranch %>%
layer_dense(units = 1, activation = "exponential", name = "q_final", kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))
# Construct Keras model
quant.model <- keras_model(inputs = c(input_pseudo_angles), outputs = c(qBranch))
# Compile the model with the tilted loss and the adam optimiser
quant.model %>%
compile(optimizer = "adam", loss = tilted_loss, run_eagerly = T)
checkpoint <- callback_model_checkpoint(filepath = paste0("runs/QR_est/qr_fit_q_",quant.level), monitor = "val_loss", verbose = 0,
save_best_only = TRUE, save_weights_only = TRUE, mode = "min", save_freq = "epoch")
n.epochs <- 100  # Set number of epochs for training
batch.size <- 512
dim(R)=c(length(R),1); dim(R.train)=c(length(R.train),1); dim(R.valid)=c(length(R.valid),1);dim(R.test)=c(length(R.test),1)
history <- quant.model %>%
fit(list(W.train), R.train, epochs = n.epochs, batch_size = batch.size, callback = list(checkpoint,
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 5)), validation_data = list(list(input_pseudo_angles = W.valid),R.valid))
# Load the best fitting model from the checkpoint save
quant.model <- load_model_weights_tf(quant.model, filepath = paste0("runs/QR_est/qr_fit_q_",quant.level))
# Then save the best model.
save_model_tf(quant.model, paste0("runs/QR_est/qr_fit_q_",quant.level))
quant.model <- tf$saved_model$load(paste0("runs/QR_est/qr_fit_q_",quant.level))
pred.quant <- k_get_value(quant.model(k_constant(W)))
mean(pred.quant>R)
init_shape <- 0.05
# Compute threshold exceedances
u <- pred.quant
# Create training and validation data
R.train <- (R - u)[-valid.inds]
W.train <- W[-valid.inds, ]
u.train <- u[-valid.inds]
R.valid <- (R - u)[valid.inds]
W.valid <- W[valid.inds, ]
u.valid <- u[valid.inds]
R.test <- (R - u)[test.inds]
W.test <- W[test.inds, ]
u.test <- u[test.inds]
W.train <- W.train[R.train > 0, ]
u.train <- u.train[R.train > 0]
R.train <- R.train[R.train > 0]
W.valid <- W.valid[R.valid > 0, ]
u.valid <- u.valid[R.valid > 0]
R.valid <- R.valid[R.valid > 0]
W.test<- W.test[R.test > 0, ]
u.test <- u.test[R.test > 0]
R.test <- R.test[R.test > 0]
dim(R.train) = c(length(R.train), 1)
dim(R.valid) = c(length(R.valid), 1)
dim(R.test) = c(length(R.test), 1)
dim(u.train) = c(length(u.train), 1)
dim(u.valid) = c(length(u.valid), 1)
dim(u.test) = c(length(u.test), 1)
input_pseudo_angles <- layer_input(shape = d, name = "input_pseudo_angles")
input_u <- layer_input(shape = d, name = "input_u")
gpd.Branch <- input_pseudo_angles %>%
layer_dense(units = gpd.scale.nunits[1], activation = activation.func, name = "gpd_dense1", kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))  #First hidden layer
if (length(gpd.scale.nunits) >= 2) {
for (i in 2:length(gpd.scale.nunits)) {
gpd.Branch <- gpd.Branch %>%
layer_dense(units = gpd.scale.nunits[i], activation = activation.func, name = paste0("gpd_dense", i), kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))  #Subsequent hidden layers
}
}
input_pseudo_angles <- layer_input(shape = d, name = "input_pseudo_angles")
input_u <- layer_input(shape = d, name = "input_u")
gpd.Branch <- input_pseudo_angles %>%
layer_dense(units = gpd.nunits[1], activation = activation.func, name = "gpd_dense1", kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))  #First hidden layer
if (length(gpd.nunits) >= 2) {
for (i in 2:length(gpd.nunits)) {
gpd.Branch <- gpd.Branch %>%
layer_dense(units = gpd.nunits[i], activation = activation.func, name = paste0("gpd_dense", i), kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,
l2 = 1e-04))  #Subsequent hidden layers
}
}
GPD_custom_activation <- function(x) {
tf$concat(c(activation_exponential(x[all_dims(),1:1]),
0.3 * activation_tanh(x[all_dims(),2:2]) - 0.2),
axis = 1L)
}
init_bias = atanh(1)
gpd.Branch <- gpd.Branch %>%
layer_dense(units = 2, activation = GPD_custom_activation, name = "gpd_final", weights = list(matrix(0,nrow = gpd.nunits[length(gpd.nunits)], ncol = 2), array(init_bias, dim = c(2))), kernel_regularizer = regularizer_l1_l2(l1 = 1e-04,l2 = 1e-04))
output <- layer_concatenate(c(gpd.Branch, input_u, input_pseudo_angles))
# Construct Keras model
GPD.model <- keras_model(inputs = c(input_pseudo_angles, input_u), outputs = output)
summary(GPD.model)
GPD.model %>%
compile(optimizer = optimizer_adam(learning_rate = l.r.vec[1]), loss = GPD_loss, run_eagerly = T)
n.epochs <- 500
batch.size = length(R.train)
checkpoint <- callback_model_checkpoint(filepath = paste0("runs/GPD_est/gpd_fit_{epoch:02d}_q_",quant.level),
monitor = "val_loss",
verbose = 0,
save_best_only = FALSE,
save_weights_only = TRUE,
mode = "min",
save_freq = "epoch")
history <- GPD.model %>%
fit(list(W.train, u.train), R.train,
epochs = n.epochs,
batch_size = batch.size,
callback = list(checkpoint,
callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 5)),
validation_data = list(list(input_pseudo_angles = W.valid,input_u = u.valid), R.valid))
